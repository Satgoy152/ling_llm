{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# I want to create a dataset for linguistic problems to then benchmark different LLMS.\n",
    "# I want to store the dataset in json format. \n",
    "\n",
    "# I am getting the data from an international linguistics competition. \n",
    "# I roughly have the idea to split the data by year, and then problems are split by context and the questions themselves\n",
    "# Can you extract the text from this pdf and format it as I described, make sure you maintain the formatting of the characters that is especially important.\n",
    "\n",
    "# For subscripts you can use latex format like S_{r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_vertexai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_anthropic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatAnthropic\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_vertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatVertexAI\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_vertexai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "model4 = ChatOpenAI(model=\"gpt-4\")\n",
    "model4o = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n",
    "model4om = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")\n",
    "model35s = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "model3o = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "model3h = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "model15p = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{ling}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain4 = prompt | model4 | output_parser\n",
    "chain4o = prompt | model4o | output_parser\n",
    "chain4om = prompt | model4om | output_parser\n",
    "chain35s = prompt | model35s | output_parser\n",
    "chain3o = prompt | model3o | output_parser\n",
    "chain3h = prompt | model3h | output_parser\n",
    "\n",
    "year = 2005\n",
    "for i in range(2015, 2019):\n",
    "    year = str(i)\n",
    "    # get prompt from txt file\n",
    "    with open(f'../iol_{year}/iol_{year}.txt', 'r') as f:\n",
    "        questions = f.read()\n",
    "    # write answers to respective text files\n",
    "    print(f\"Processing year {year}\")\n",
    "    # print(\"GPT-4\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_gpt4.txt', 'w') as f:\n",
    "    #     f.write(chain4.invoke({\"ling\": questions}))\n",
    "    # print(\"GPT-4o\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_gpt4o.txt', 'w') as f:\n",
    "    #     f.write(chain4o.invoke({\"ling\": questions}))\n",
    "    # print(\"GPT-4om\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_gpt4om.txt', 'w') as f:\n",
    "    #     f.write(chain4om.invoke({\"ling\": questions}))\n",
    "    # print(\"Claude 3.5s\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_claude3.5s.txt', 'w') as f:\n",
    "    #     f.write(chain35s.invoke({\"ling\": questions}))\n",
    "    # print(\"Claude 3o\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_claude3o.txt', 'w') as f:\n",
    "    #     f.write(chain3o.invoke({\"ling\": questions}))\n",
    "    # print(\"Claude 3h\")\n",
    "    # with open(f'../iol_{year}/iol_{year}_claude3h.txt', 'w') as f:\n",
    "    #     f.write(chain3h.invoke({\"ling\": questions}))\n",
    "    print(\"Gemeni 1.5p\")\n",
    "    with open(f'../iol_{year}/iol_{year}_gemeni1.5p.txt', 'w') as f:\n",
    "        f.write(model15p.generate_content(questions).candidates[0].content.parts[0].text)\n",
    "    print(\"Done processing year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o\n",
      "GPT-4om\n",
      "Claude 3.5s\n",
      "Claude 3o\n",
      "Claude 3h\n",
      "Gemini 1.5p\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "model4 = ChatOpenAI(model=\"gpt-4\", temperature=0.7, max_tokens=3096)\n",
    "model4o = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0.7, max_tokens=4096)\n",
    "model4om = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\", temperature=0.7, max_tokens=4096)\n",
    "model35s = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.7, max_tokens=4096)\n",
    "model3o = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0.7, max_tokens=4096)\n",
    "model3h = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0.7, max_tokens=4096)\n",
    "model15p = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{ling}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain4 = prompt | model4 | output_parser\n",
    "chain4o = prompt | model4o | output_parser\n",
    "chain4om = prompt | model4om | output_parser\n",
    "chain35s = prompt | model35s | output_parser\n",
    "chain3o = prompt | model3o | output_parser\n",
    "chain3h = prompt | model3h | output_parser\n",
    "\n",
    "\n",
    "with open(f'../iol_samples/text/sample_probs_text.txt', 'r') as f:\n",
    "    questions = f.read()\n",
    "\n",
    "path = '../iol_samples/text/'\n",
    "# write answers to respective text files\n",
    "# print(\"GPT-4\")\n",
    "# with open(f'{path}gpt4/response.txt', 'w') as f:\n",
    "#     f.write(chain4.invoke({\"ling\": questions}))\n",
    "print(\"GPT-4o\")\n",
    "with open(f'{path}gpt4o/response.txt', 'w') as f:\n",
    "    f.write(chain4o.invoke({\"ling\": questions}))\n",
    "print(\"GPT-4om\")\n",
    "with open(f'{path}gpt4om/response.txt', 'w') as f:\n",
    "    f.write(chain4om.invoke({\"ling\": questions}))\n",
    "print(\"Claude 3.5s\")\n",
    "with open(f'{path}claude3.5s/response.txt', 'w') as f:\n",
    "    f.write(chain35s.invoke({\"ling\": questions}))\n",
    "print(\"Claude 3o\")\n",
    "with open(f'{path}claude3o/response.txt', 'w') as f:\n",
    "    f.write(chain3o.invoke({\"ling\": questions}))\n",
    "print(\"Claude 3h\")\n",
    "with open(f'{path}claude3h/response.txt', 'w') as f:\n",
    "    f.write(chain3h.invoke({\"ling\": questions}))\n",
    "print(\"Gemini 1.5p\")\n",
    "with open(f'{path}gemini1.5p/response.txt', 'w') as f:\n",
    "    f.write(model15p.generate_content(questions).candidates[0].content.parts[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2004\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! What can I do for you today? \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"../iol_2011/iol-2011-indiv-prob.en-us.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[0]\n",
    "text = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ninth International Olympiad in LinguisticsPittsburgh (United States of America), July 24–31, 2011Individual Contest ProblemsProblem #1 (20 points).Given are verb forms of the Menominee language as well as their Englishtranslations:kew¯ æpeqtaqwe1+2beginkaw¯ ahamhe fells it by toolnep¯ ıtohnæmI walk here (to this place)k¯ eskenamhe breaks it through by handpahk¯ æsamhe cuts it oﬀkek¯ ætohnæqwe1+2walk outp¯ ıtenamhe passes it herekew¯ æp¯ anæhkæqwe1+2begin to digtaw¯ æsamhe cuts a hole in itnek¯ ætahanI pry it out by toolp¯ ahkahamhe opens it by tool (by raising a lid or opening a door)kek¯ eskahtæqwe1+2bite it throughwack¯ ohnæwhe walks roundabout, by a detournew¯ ackesanI cut around itket¯ ænamhe takes it out by handket¯ awahtæqwe1+2bite, gnaw a hole in itw¯ æpohnæwhe begins walkingnek¯ aweqtamI lie downp¯ ahkeqtawhe opens upkep¯ ıtahtæqwe1+2come eating it; we1+2bring it in our mouthsnek¯ aw¯ ahpemI fall over laughing(a)Translate into English:kek¯ eskahæq,nep¯ ahkenan,w¯ æp¯ ahpew. If in some cases you believethat more than one translation is possible, give them all.(b)Translate into Menominee:•I begin to eat it•we1+2lay it ﬂat by hand•he digs a hole•he walks out!△The Menominee Indians live in Wisconsin, USA. They number 5 000–10 000 people, but the epony-mous language of the Algonquian family is only spoken by a few dozen of the oldest among them,although eﬀort has been put lately into expanding its teaching and use.“we1+2” = ‘we and you’.æ≈aincrack,c=chinchurch,qis a consonant (the so-called glottalstop). The mark“¯”denotes vowel length.—Ivan Derzhanski\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_mentor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
